#!/usr/bin/env python3

import os
import re
import glob
import pandas as pd
import argparse
import json
import warnings
from pathlib import Path
from urllib.parse import urljoin, urlparse

from constants import DOMAINS, get_commands
from spinner import Spinner
from state import CLIState

from dsm_utils import (
    get_latest_dsm_file,
    load_spreadsheet,
    get_existing_url,
    get_proposed_url,
    get_column_value,
    get_row_data,
)
from page_extractor import (
    retrieve_page_data,
    display_page_data,
    extract_links_from_page,
    extract_embeds_from_page,
    normalize_url,
    check_status_code,
)
from migration import migrate
import requests
from bs4 import BeautifulSoup

from utils import debug_print, set_debug

# Toggle debugging at top level (default: on)
warnings.filterwarnings("ignore", category=UserWarning, module="openpyxl")

# Global state instance
state = CLIState()

# Get commands from constants
COMMANDS = get_commands(state)

# Constants for Excel parsing
HEADER_ROW = 3  # zero-based index where actual header resides

# Directory where DSM files live
DSM_DIR = Path(".")
CACHE_DIR = Path("migration_cache")
CACHE_DIR.mkdir(exist_ok=True)


# def check_page_workflow(excel):
#     """Legacy workflow function - maintained for compatibility."""
#     print(
#         "‚ö†Ô∏è  This is the legacy workflow. Consider using the new state-based commands:"
#     )
#     print("   1. set URL <your_url>")
#     print("   2. check")
#     print("   3. show page")
#     print()

#     choice = input("Continue with legacy workflow? (y/n) > ").strip().lower()
#     if choice != "y":
#         print("üí° Use 'help' to see new commands")
#         return

#     choice = input("Choose source: (u) Page URL, (s) Spreadsheet > ").strip().lower()
#     debug_print(f"User chose source: {choice}")
#     if choice == "s":
#         sheets = list_domains(excel)
#         for i, name in enumerate(sheets, start=1):
#             print(f"({i}) {name}")
#         idx = int(input("Select domain by number > ").strip())
#         domain = sheets[idx - 1]
#         debug_print(f"Selected domain: {domain}")
#         df = excel.parse(domain, header=HEADER_ROW)
#         debug_print(f"Loaded DataFrame with shape: {df.shape}")
#         row_num = int(input("Enter row number > ").strip())
#         url = get_existing_url(df, row_num)
#         if not url:
#             print("Could not find URL for that row.")
#             return
#         warn = count_http(url) > 1
#         print(f"URL: {url[:60]}{'...' if len(url) > 60 else ''}")
#         if warn:
#             print("WARNING: Multiple URLs detected in this cell.")
#         action = (
#             input("Options: (g) Get page data, (m) More details > ").strip().lower()
#         )
#         debug_print(f"User selected action: {action}")
#         if action == "g":
#             # Ask for CSS selector
#             selector = input("CSS selector (default: #main): ").strip() or "#main"
#             print(f"Loading page data using selector '{selector}'...")

#             try:
#                 data = retrieve_page_data(url, selector)
#                 cache_file = CACHE_DIR / f"page_{domain}_{row_num}.json"

#                 # Save to JSON file with proper formatting
#                 with open(cache_file, "w", encoding="utf-8") as f:
#                     json.dump(data, f, indent=2, ensure_ascii=False)

#                 print(f"‚úÖ Data cached to {cache_file}")

#                 if "error" in data:
#                     print(f"‚ùå Failed to extract data: {data['error']}")
#                     return

#                 # Show summary
#                 links_count = len(data.get("links", []))
#                 pdfs_count = len(data.get("pdfs", []))
#                 embeds_count = len(data.get("embeds", []))

#                 print(
#                     f"üìä Summary: {links_count} links, {pdfs_count} PDFs, {embeds_count} embeds found"
#                 )

#                 next_act = (
#                     input("Options: (s) Show page data, (m) Migrate page > ")
#                     .strip()
#                     .lower()
#                 )
#                 debug_print(f"Next action: {next_act}")
#                 if next_act == "s":
#                     display_page_data(data)
#                 elif next_act == "m":
#                     print("Migrating page...")
#                     migrate(url, data, domain, row_num)
#             except Exception as e:
#                 print(f"‚ùå Error during page extraction: {e}")
#                 debug_print(f"Full error: {e}")
#         else:
#             print("Detail view not implemented.")
#     else:
#         # Direct URL input
#         url = input("Enter page URL > ").strip()
#         if not url:
#             print("No URL provided.")
#             return

#         url = normalize_url(url)
#         warn = count_http(url) > 1
#         print(f"URL: {url[:60]}{'...' if len(url) > 60 else ''}")
#         if warn:
#             print("WARNING: Multiple URLs detected in this input.")

#         selector = input("CSS selector (default: #main): ").strip() or "#main"
#         print(f"Loading page data using selector '{selector}'...")

#         try:
#             data = retrieve_page_data(url, selector)

#             # Create a simple cache filename for direct URLs
#             url_safe = re.sub(r"[^\w\-_.]", "_", url)[:50]
#             cache_file = CACHE_DIR / f"page_direct_{url_safe}.json"

#             # Save to JSON file with proper formatting
#             with open(cache_file, "w", encoding="utf-8") as f:
#                 json.dump(data, f, indent=2, ensure_ascii=False)

#             print(f"‚úÖ Data cached to {cache_file}")

#             if "error" in data:
#                 print(f"‚ùå Failed to extract data: {data['error']}")
#                 return

#             # Show summary
#             links_count = len(data.get("links", []))
#             pdfs_count = len(data.get("pdfs", []))
#             embeds_count = len(data.get("embeds", []))

#             print(
#                 f"üìä Summary: {links_count} links, {pdfs_count} PDFs, {embeds_count} embeds found"
#             )

#             next_act = (
#                 input("Options: (s) Show page data, (m) Migrate page > ")
#                 .strip()
#                 .lower()
#             )
#             if next_act == "s":
#                 display_page_data(data)
#             elif next_act == "m":
#                 print("Migrating page...")
#                 migrate(url, data, domain, row_num)
#         except Exception as e:
#             print(f"‚ùå Error during page extraction: {e}")
#             debug_print(f"Full error: {e}")


def normalize_url(url):
    """Ensure the URL has a scheme."""
    parsed = urlparse(url)
    if not parsed.scheme:
        return "http://" + url
    return url


def check_status_code(url):
    """Check the HTTP status code of a URL."""
    try:
        response = requests.head(url, allow_redirects=True, timeout=10)
        return str(response.status_code)
    except requests.RequestException:
        return "0"


def extract_links_from_page(url, selector="#main"):
    """
    Fetch the page, parse HTML, and return list of (text, absolute href, status_code).
    Also separates PDF links from regular links.
    """
    debug_print(f"Fetching page: {url}")
    debug_print(f"Using CSS selector: {selector}")

    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, "html.parser")
        container = soup.select_one(selector)

        if not container:
            debug_print(
                f"Warning: No element found matching selector '{selector}', falling back to entire page"
            )
            container = soup

        anchors = container.find_all("a", href=True)
        debug_print(f"Found {len(anchors)} anchor tags")

        links = []
        pdfs = []

        for a in anchors:
            text = a.get_text(strip=True)
            href = urljoin(response.url, a["href"])
            debug_print(
                f"Processing link: {text[:50]}{'...' if len(text) > 50 else ''} -> {href}"
            )
            status_code = check_status_code(href)

            # Check if this is a PDF link
            if href.lower().endswith(".pdf"):
                pdfs.append((text, href, status_code))
                debug_print(f"  -> Categorized as PDF")
            else:
                links.append((text, href, status_code))
                debug_print(f"  -> Categorized as regular link")

        return links, pdfs

    except requests.RequestException as e:
        debug_print(f"Error fetching page: {e}")
        raise


def extract_embeds_from_page(soup, selector="#main"):
    """Extract Vimeo embedded content from within the specified container."""
    embeds = []

    # Get the same container used for link extraction
    container = soup.select_one(selector)
    if not container:
        debug_print(
            f"Warning: No element found matching selector '{selector}', falling back to entire page for embeds"
        )
        container = soup

    # Find iframes with Vimeo content only
    for iframe in container.find_all("iframe", src=True):
        src = iframe.get("src", "")
        if "vimeo" in src.lower():
            title = (
                iframe.get("title", "") or iframe.get_text(strip=True) or "Vimeo Video"
            )
            embeds.append(("vimeo", title, src))
            debug_print(f"Found Vimeo embed: {title}")

    return embeds


# Command parsing and execution
def parse_command(input_line):
    """Parse command line input into command and arguments."""
    parts = input_line.strip().split()
    if not parts:
        return None, []

    command = parts[0].lower()
    args = parts[1:] if len(parts) > 1 else []
    return command, args


def execute_command(command, args):
    """Execute a command with given arguments."""
    if command in COMMANDS:
        try:
            COMMANDS[command](args)
        except KeyboardInterrupt:
            print("\n‚ö†Ô∏è  Command interrupted")
        except Exception as e:
            print(f"‚ùå Command error: {e}")
            from utils import DEBUG

            if DEBUG:
                import traceback

                traceback.print_exc()
    else:
        print(f"‚ùå Unknown command: {command}")
        print("üí° Type 'help' for available commands")


def main():
    parser = argparse.ArgumentParser(
        description="Linker CLI POC - State-based Framework"
    )
    parser.add_argument(
        "--debug", dest="debug", action="store_true", help="Enable debug output"
    )
    parser.add_argument(
        "--no-debug", dest="debug", action="store_false", help="Disable debug output"
    )
    parser.add_argument("--url", help="Set initial URL")
    parser.add_argument("--selector", default="#main", help="Set initial CSS selector")
    parser.set_defaults(debug=True)
    args = parser.parse_args()

    # Set debug mode in utils
    set_debug(args.debug)
    debug_print(f"Debugging is {'enabled' if args.debug else 'disabled'}")

    # Set initial state from command line args
    if args.url:
        state.set_variable("URL", args.url)
    if args.selector:
        state.set_variable("SELECTOR", args.selector)

    # Try to auto-load the latest DSM file
    dsm_file = get_latest_dsm_file()
    if dsm_file:
        try:
            state.excel_data = load_spreadsheet(dsm_file)
            state.set_variable("DSM_FILE", dsm_file)
            debug_print(f"Auto-loaded DSM file: {dsm_file}")
        except Exception as e:
            debug_print(f"Failed to auto-load DSM file: {e}")

    print("üîó Welcome to Linker CLI v2.0 - State-based Framework")
    print("üí° Type 'help' for available commands")

    # Show initial state if any variables are set
    if any(state.variables.values()):
        print("\nüìã Initial state:")
        state.list_variables()

    while True:
        try:
            # Create a prompt similar to Metasploit
            url_indicator = (
                f"({state.get_variable('URL')[:30]}...)"
                if state.get_variable("URL")
                else "(no url)"
            )
            prompt = f"linker {url_indicator} > "

            user_input = input(prompt).strip()
            if not user_input:
                continue

            command, args = parse_command(user_input)
            if command:
                debug_print(f"Executing command: {command} with args: {args}")
                execute_command(command, args)

        except KeyboardInterrupt:
            print("\n‚ö†Ô∏è  Use 'exit' or 'quit' to leave the application")
        except EOFError:
            print("\nGoodbye.")
            break


if __name__ == "__main__":
    main()
